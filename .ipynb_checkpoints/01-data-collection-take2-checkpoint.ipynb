{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L. Minter\n",
    "### October 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook 01: Data Collection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement: \n",
    "We want to build a marketing tool to help local businesses understand which subreddits to target for advertisement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'subreddit':'Seattle',\n",
    "    'metadata':'True',\n",
    "    'size':100,\n",
    "    'before':1635439118\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get( url = base_url, params = params)\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = res.json()['data']\n",
    "metadata = res.json()['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>...</th>\n",
       "      <th>url_overridden_by_dest</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>is_gallery</th>\n",
       "      <th>gallery_data</th>\n",
       "      <th>media_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>galumphix</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_s4xtg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Automatic_Man52</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_cqpcdsow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>https://datastudio.google.com/reporting/314b52...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>phillipsn21</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_59m8pcz0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moving</td>\n",
       "      <td>Moving / Visiting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>wsdot</td>\n",
       "      <td>flair verified</td>\n",
       "      <td>[{'e': 'text', 't': 'WA State Dept of Transpor...</td>\n",
       "      <td>WA State Dept of Transportation</td>\n",
       "      <td>richtext</td>\n",
       "      <td>t2_4upd9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>dark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>gharrity</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_6lx0n</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/poli...</td>\n",
       "      <td>flair</td>\n",
       "      <td>Politics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d03c04ee-412a-11e8-88cf-0e80e220ed5c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments           author author_flair_css_class  \\\n",
       "0            []                False        galumphix                   None   \n",
       "1            []                False  Automatic_Man52                   None   \n",
       "2            []                False      phillipsn21                   None   \n",
       "3            []                False            wsdot         flair verified   \n",
       "4            []                False         gharrity                   None   \n",
       "\n",
       "                               author_flair_richtext  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [{'e': 'text', 't': 'WA State Dept of Transpor...   \n",
       "4                                                 []   \n",
       "\n",
       "                 author_flair_text author_flair_type author_fullname  \\\n",
       "0                             None              text        t2_s4xtg   \n",
       "1                             None              text     t2_cqpcdsow   \n",
       "2                             None              text     t2_59m8pcz0   \n",
       "3  WA State Dept of Transportation          richtext        t2_4upd9   \n",
       "4                             None              text        t2_6lx0n   \n",
       "\n",
       "   author_is_blocked  author_patreon_flair  ...  \\\n",
       "0              False                 False  ...   \n",
       "1              False                 False  ...   \n",
       "2              False                 False  ...   \n",
       "3              False                 False  ...   \n",
       "4              False                 False  ...   \n",
       "\n",
       "                              url_overridden_by_dest link_flair_css_class  \\\n",
       "0                                                NaN                  NaN   \n",
       "1  https://datastudio.google.com/reporting/314b52...                  NaN   \n",
       "2                                                NaN               moving   \n",
       "3                                                NaN                  NaN   \n",
       "4  https://www.seattletimes.com/seattle-news/poli...                flair   \n",
       "\n",
       "     link_flair_text  author_flair_background_color  author_flair_text_color  \\\n",
       "0                NaN                            NaN                      NaN   \n",
       "1                NaN                            NaN                      NaN   \n",
       "2  Moving / Visiting                            NaN                      NaN   \n",
       "3                NaN                                                    dark   \n",
       "4           Politics                            NaN                      NaN   \n",
       "\n",
       "                 link_flair_template_id author_flair_template_id is_gallery  \\\n",
       "0                                   NaN                      NaN        NaN   \n",
       "1                                   NaN                      NaN        NaN   \n",
       "2                                   NaN                      NaN        NaN   \n",
       "3                                   NaN                      NaN        NaN   \n",
       "4  d03c04ee-412a-11e8-88cf-0e80e220ed5c                      NaN        NaN   \n",
       "\n",
       "  gallery_data  media_metadata  \n",
       "0          NaN             NaN  \n",
       "1          NaN             NaN  \n",
       "2          NaN             NaN  \n",
       "3          NaN             NaN  \n",
       "4          NaN             NaN  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(posts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get a request given the previous data frame\n",
    "# inputs: df, output: df with more data\n",
    "\n",
    "def get_more_posts(dataframe):\n",
    "    utc_cutoff = dataframe['created_utc'].min()\n",
    "    params = {\n",
    "        'subreddit':'Seattle',\n",
    "        'metadata':'True',\n",
    "        'size':100,\n",
    "        'before': utc_cutoff\n",
    "    }\n",
    "    #print(utc_cutoff)\n",
    "    \n",
    "    req = requests.get(url=base_url,params=params)\n",
    "    \n",
    "    dataframe_new = pd.DataFrame(req.json()['data'])\n",
    "    time.sleep(1)\n",
    "    return pd.concat([dataframe,dataframe_new])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get as much data as possible.  Eric says at least 1k each.  Possibly more...\n",
    "test_df = get_more_posts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    test_df = get_more_posts(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    test_df = get_more_posts(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154172"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./seattle_posts.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SeattleWA\n",
    "params_SeattleWA = {\n",
    "    'subreddit':'SeattleWA',\n",
    "    'metadata':'True',\n",
    "    'size':100,\n",
    "    'before':1635439118\n",
    "}\n",
    "res = requests.get( url = base_url, params = params_SeattleWA)\n",
    "res.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = res.json()['data']\n",
    "metadata = res.json()['metadata']\n",
    "posts = res.json()['data']\n",
    "metadata = res.json()['metadata']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'after': None,\n",
       " 'agg_size': 100,\n",
       " 'api_version': '3.0',\n",
       " 'before': 1635439118,\n",
       " 'es_query': {'query': {'bool': {'filter': {'bool': {'must': [{'terms': {'subreddit': ['seattlewa']}},\n",
       "       {'range': {'created_utc': {'lt': 1635439118}}}],\n",
       "      'should': []}},\n",
       "    'must_not': []}},\n",
       "  'size': 100,\n",
       "  'sort': {'created_utc': 'desc'}},\n",
       " 'execution_time_milliseconds': 45.83,\n",
       " 'index': 'rs',\n",
       " 'metadata': 'True',\n",
       " 'ranges': [{'range': {'created_utc': {'lt': 1635439118}}}],\n",
       " 'results_returned': 100,\n",
       " 'shards': {'failed': 0, 'skipped': 0, 'successful': 20, 'total': 24},\n",
       " 'size': 100,\n",
       " 'sort': 'desc',\n",
       " 'sort_type': 'created_utc',\n",
       " 'subreddit': ['SeattleWA'],\n",
       " 'timed_out': False,\n",
       " 'total_results': 79939}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>...</th>\n",
       "      <th>poll_data</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>gallery_data</th>\n",
       "      <th>is_gallery</th>\n",
       "      <th>media_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>gehnrahl</td>\n",
       "      <td>#edeff1</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': 'Nur Bahnhof verstehen'}]</td>\n",
       "      <td>89e2036c-d5bd-11e9-9c3f-0e58b79bcec4</td>\n",
       "      <td>Nur Bahnhof verstehen</td>\n",
       "      <td>dark</td>\n",
       "      <td>richtext</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>gehnrahl</td>\n",
       "      <td>#edeff1</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': 'Nur Bahnhof verstehen'}]</td>\n",
       "      <td>89e2036c-d5bd-11e9-9c3f-0e58b79bcec4</td>\n",
       "      <td>Nur Bahnhof verstehen</td>\n",
       "      <td>dark</td>\n",
       "      <td>richtext</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>gehnrahl</td>\n",
       "      <td>#edeff1</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': 'Nur Bahnhof verstehen'}]</td>\n",
       "      <td>89e2036c-d5bd-11e9-9c3f-0e58b79bcec4</td>\n",
       "      <td>Nur Bahnhof verstehen</td>\n",
       "      <td>dark</td>\n",
       "      <td>richtext</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>gehnrahl</td>\n",
       "      <td>#edeff1</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': 'Nur Bahnhof verstehen'}]</td>\n",
       "      <td>89e2036c-d5bd-11e9-9c3f-0e58b79bcec4</td>\n",
       "      <td>Nur Bahnhof verstehen</td>\n",
       "      <td>dark</td>\n",
       "      <td>richtext</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>gehnrahl</td>\n",
       "      <td>#edeff1</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': 'Nur Bahnhof verstehen'}]</td>\n",
       "      <td>89e2036c-d5bd-11e9-9c3f-0e58b79bcec4</td>\n",
       "      <td>Nur Bahnhof verstehen</td>\n",
       "      <td>dark</td>\n",
       "      <td>richtext</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments    author author_flair_background_color  \\\n",
       "0            []                False  gehnrahl                       #edeff1   \n",
       "1            []                False  gehnrahl                       #edeff1   \n",
       "2            []                False  gehnrahl                       #edeff1   \n",
       "3            []                False  gehnrahl                       #edeff1   \n",
       "4            []                False  gehnrahl                       #edeff1   \n",
       "\n",
       "  author_flair_css_class                          author_flair_richtext  \\\n",
       "0                   None  [{'e': 'text', 't': 'Nur Bahnhof verstehen'}]   \n",
       "1                   None  [{'e': 'text', 't': 'Nur Bahnhof verstehen'}]   \n",
       "2                   None  [{'e': 'text', 't': 'Nur Bahnhof verstehen'}]   \n",
       "3                   None  [{'e': 'text', 't': 'Nur Bahnhof verstehen'}]   \n",
       "4                   None  [{'e': 'text', 't': 'Nur Bahnhof verstehen'}]   \n",
       "\n",
       "               author_flair_template_id      author_flair_text  \\\n",
       "0  89e2036c-d5bd-11e9-9c3f-0e58b79bcec4  Nur Bahnhof verstehen   \n",
       "1  89e2036c-d5bd-11e9-9c3f-0e58b79bcec4  Nur Bahnhof verstehen   \n",
       "2  89e2036c-d5bd-11e9-9c3f-0e58b79bcec4  Nur Bahnhof verstehen   \n",
       "3  89e2036c-d5bd-11e9-9c3f-0e58b79bcec4  Nur Bahnhof verstehen   \n",
       "4  89e2036c-d5bd-11e9-9c3f-0e58b79bcec4  Nur Bahnhof verstehen   \n",
       "\n",
       "  author_flair_text_color author_flair_type  ... poll_data  crosspost_parent  \\\n",
       "0                    dark          richtext  ...       NaN               NaN   \n",
       "1                    dark          richtext  ...       NaN               NaN   \n",
       "2                    dark          richtext  ...       NaN               NaN   \n",
       "3                    dark          richtext  ...       NaN               NaN   \n",
       "4                    dark          richtext  ...       NaN               NaN   \n",
       "\n",
       "   crosspost_parent_list  media media_embed  secure_media  secure_media_embed  \\\n",
       "0                    NaN    NaN         NaN           NaN                 NaN   \n",
       "1                    NaN    NaN         NaN           NaN                 NaN   \n",
       "2                    NaN    NaN         NaN           NaN                 NaN   \n",
       "3                    NaN    NaN         NaN           NaN                 NaN   \n",
       "4                    NaN    NaN         NaN           NaN                 NaN   \n",
       "\n",
       "   gallery_data is_gallery media_metadata  \n",
       "0           NaN        NaN            NaN  \n",
       "1           NaN        NaN            NaN  \n",
       "2           NaN        NaN            NaN  \n",
       "3           NaN        NaN            NaN  \n",
       "4           NaN        NaN            NaN  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(posts)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_subs(dataframe,subreddit):\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    \n",
    "    #base cutoff on the last time in the current dataframe\n",
    "    utc_cutoff = dataframe['created_utc'].min()\n",
    "    \n",
    "    params = {\n",
    "        'subreddit':subreddit,\n",
    "        'metadata':'True',\n",
    "        'size':100,\n",
    "        'before': utc_cutoff\n",
    "    }\n",
    "    \n",
    "    req = requests.get(url=base_url,params=params)\n",
    "    \n",
    "    dataframe_new = pd.DataFrame(req.json()['data'])\n",
    "    \n",
    "    #take a short break\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return pd.concat([dataframe,dataframe_new])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2 = get_more_subs(df2,'SeattleWA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    test_df2 = get_more_subs(test_df2,'SeattleWA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79856"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'after': None,\n",
       " 'agg_size': 100,\n",
       " 'api_version': '3.0',\n",
       " 'before': 1635439118,\n",
       " 'es_query': {'query': {'bool': {'filter': {'bool': {'must': [{'terms': {'subreddit': ['seattlewa']}},\n",
       "       {'range': {'created_utc': {'lt': 1635439118}}}],\n",
       "      'should': []}},\n",
       "    'must_not': []}},\n",
       "  'size': 100,\n",
       "  'sort': {'created_utc': 'desc'}},\n",
       " 'execution_time_milliseconds': 45.83,\n",
       " 'index': 'rs',\n",
       " 'metadata': 'True',\n",
       " 'ranges': [{'range': {'created_utc': {'lt': 1635439118}}}],\n",
       " 'results_returned': 100,\n",
       " 'shards': {'failed': 0, 'skipped': 0, 'successful': 20, 'total': 24},\n",
       " 'size': 100,\n",
       " 'sort': 'desc',\n",
       " 'sort_type': 'created_utc',\n",
       " 'subreddit': ['SeattleWA'],\n",
       " 'timed_out': False,\n",
       " 'total_results': 79939}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.to_csv('./seattlewa_posts.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_posts(subreddit_name):\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    starting_utc = 1635439118\n",
    "    \n",
    "    params = {\n",
    "        'subreddit':subreddit_name,\n",
    "        'size':100,\n",
    "        'before':starting_utc\n",
    "        }\n",
    "    \n",
    "    #get initial data frame for first 100 submissions\n",
    "    df = pd.DataFrame(requests.get( url = base_url, params = params).json()['data'])\n",
    "    df.to_csv(f'./data/{subreddit_name}_{starting_utc}.csv')\n",
    "    \n",
    "    #then loop to get the rest of them\n",
    "    for i in range(1500):\n",
    "        if i%10 == 0: print('processing', i)\n",
    "        params['before']=df['created_utc'].min() #updated UTC cutoff using the data we already have\n",
    "        #print(df['created_utc'].min())\n",
    "        res = requests.get( url = base_url, params = params)\n",
    "        if res.status_code==200:\n",
    "            posts = res.json()['data']\n",
    "            df = pd.DataFrame(posts)\n",
    "            utc = params['before']\n",
    "            df.to_csv(f'./data/{subreddit_name}/posts_{subreddit_name}_{utc}.csv',index = False)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0\n",
      "processing 10\n",
      "processing 20\n",
      "processing 30\n",
      "processing 40\n",
      "processing 50\n",
      "processing 60\n",
      "processing 70\n",
      "processing 80\n",
      "processing 90\n",
      "processing 100\n",
      "processing 110\n",
      "processing 120\n",
      "processing 130\n",
      "processing 140\n",
      "processing 150\n",
      "processing 160\n",
      "processing 170\n",
      "processing 180\n",
      "processing 190\n",
      "processing 200\n",
      "processing 210\n",
      "processing 220\n",
      "processing 230\n",
      "processing 240\n",
      "processing 250\n",
      "processing 260\n",
      "processing 270\n",
      "processing 280\n",
      "processing 290\n",
      "processing 300\n",
      "processing 310\n",
      "processing 320\n",
      "processing 330\n",
      "processing 340\n",
      "processing 350\n",
      "processing 360\n",
      "processing 370\n",
      "processing 380\n",
      "processing 390\n",
      "processing 400\n",
      "processing 410\n",
      "processing 420\n",
      "processing 430\n",
      "processing 440\n",
      "processing 450\n",
      "processing 460\n",
      "processing 470\n",
      "processing 480\n",
      "processing 490\n",
      "processing 500\n",
      "processing 510\n",
      "processing 520\n",
      "processing 530\n",
      "processing 540\n",
      "processing 550\n",
      "processing 560\n",
      "processing 570\n",
      "processing 580\n",
      "processing 590\n",
      "processing 600\n",
      "processing 610\n",
      "processing 620\n",
      "processing 630\n",
      "processing 640\n",
      "processing 650\n",
      "processing 660\n",
      "processing 670\n",
      "processing 680\n",
      "processing 690\n",
      "processing 700\n",
      "processing 710\n",
      "processing 720\n",
      "processing 730\n",
      "processing 740\n",
      "processing 750\n",
      "processing 760\n",
      "processing 770\n",
      "processing 780\n",
      "processing 790\n",
      "processing 800\n",
      "processing 810\n",
      "processing 820\n",
      "processing 830\n",
      "processing 840\n",
      "processing 850\n",
      "processing 860\n",
      "processing 870\n",
      "processing 880\n",
      "processing 890\n",
      "processing 900\n",
      "processing 910\n",
      "processing 920\n",
      "processing 930\n",
      "processing 940\n",
      "processing 950\n",
      "processing 960\n",
      "processing 970\n",
      "processing 980\n",
      "processing 990\n",
      "processing 1000\n",
      "processing 1010\n",
      "processing 1020\n",
      "processing 1030\n",
      "processing 1040\n",
      "processing 1050\n",
      "processing 1060\n",
      "processing 1070\n",
      "processing 1080\n",
      "processing 1090\n",
      "processing 1100\n",
      "processing 1110\n",
      "processing 1120\n",
      "processing 1130\n",
      "processing 1140\n",
      "processing 1150\n",
      "processing 1160\n",
      "processing 1170\n",
      "processing 1180\n",
      "processing 1190\n",
      "processing 1200\n",
      "processing 1210\n",
      "processing 1220\n",
      "processing 1230\n",
      "processing 1240\n",
      "processing 1250\n",
      "processing 1260\n",
      "processing 1270\n",
      "processing 1280\n",
      "processing 1290\n",
      "processing 1300\n",
      "processing 1310\n",
      "processing 1320\n",
      "processing 1330\n",
      "processing 1340\n",
      "processing 1350\n",
      "processing 1360\n",
      "processing 1370\n",
      "processing 1380\n",
      "processing 1390\n",
      "processing 1400\n",
      "processing 1410\n",
      "processing 1420\n",
      "processing 1430\n",
      "processing 1440\n",
      "processing 1450\n",
      "processing 1460\n",
      "processing 1470\n",
      "processing 1480\n",
      "processing 1490\n"
     ]
    }
   ],
   "source": [
    "get_all_posts('SeattleWA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0\n",
      "processing 10\n",
      "processing 20\n",
      "processing 30\n",
      "processing 40\n",
      "processing 50\n",
      "processing 60\n",
      "processing 70\n",
      "processing 80\n",
      "processing 90\n",
      "processing 100\n",
      "processing 110\n",
      "processing 120\n",
      "processing 130\n",
      "processing 140\n",
      "processing 150\n",
      "processing 160\n",
      "processing 170\n",
      "processing 180\n",
      "processing 190\n",
      "processing 200\n",
      "processing 210\n",
      "processing 220\n",
      "processing 230\n",
      "processing 240\n",
      "processing 250\n",
      "processing 260\n",
      "processing 270\n",
      "processing 280\n",
      "processing 290\n",
      "processing 300\n",
      "processing 310\n",
      "processing 320\n",
      "processing 330\n",
      "processing 340\n",
      "processing 350\n",
      "processing 360\n",
      "processing 370\n",
      "processing 380\n",
      "processing 390\n",
      "processing 400\n",
      "processing 410\n",
      "processing 420\n",
      "processing 430\n",
      "processing 440\n",
      "processing 450\n",
      "processing 460\n",
      "processing 470\n",
      "processing 480\n",
      "processing 490\n",
      "processing 500\n",
      "processing 510\n",
      "processing 520\n",
      "processing 530\n",
      "processing 540\n",
      "processing 550\n",
      "processing 560\n",
      "processing 570\n",
      "processing 580\n",
      "processing 590\n",
      "processing 600\n",
      "processing 610\n",
      "processing 620\n",
      "processing 630\n",
      "processing 640\n",
      "processing 650\n",
      "processing 660\n",
      "processing 670\n",
      "processing 680\n",
      "processing 690\n",
      "processing 700\n",
      "processing 710\n",
      "processing 720\n",
      "processing 730\n",
      "processing 740\n",
      "processing 750\n",
      "processing 760\n",
      "processing 770\n",
      "processing 780\n",
      "processing 790\n",
      "processing 800\n",
      "processing 810\n",
      "processing 820\n",
      "processing 830\n",
      "processing 840\n",
      "processing 850\n",
      "processing 860\n",
      "processing 870\n",
      "processing 880\n",
      "processing 890\n",
      "processing 900\n",
      "processing 910\n",
      "processing 920\n",
      "processing 930\n",
      "processing 940\n",
      "processing 950\n",
      "processing 960\n",
      "processing 970\n",
      "processing 980\n",
      "processing 990\n",
      "processing 1000\n",
      "processing 1010\n",
      "processing 1020\n",
      "processing 1030\n",
      "processing 1040\n",
      "processing 1050\n",
      "processing 1060\n",
      "processing 1070\n",
      "processing 1080\n",
      "processing 1090\n",
      "processing 1100\n",
      "processing 1110\n",
      "processing 1120\n",
      "processing 1130\n",
      "processing 1140\n",
      "processing 1150\n",
      "processing 1160\n",
      "processing 1170\n",
      "processing 1180\n",
      "processing 1190\n",
      "processing 1200\n",
      "processing 1210\n",
      "processing 1220\n",
      "processing 1230\n",
      "processing 1240\n",
      "processing 1250\n",
      "processing 1260\n",
      "processing 1270\n",
      "processing 1280\n",
      "processing 1290\n",
      "processing 1300\n",
      "processing 1310\n",
      "processing 1320\n",
      "processing 1330\n",
      "processing 1340\n",
      "processing 1350\n",
      "processing 1360\n",
      "processing 1370\n",
      "processing 1380\n",
      "processing 1390\n",
      "processing 1400\n",
      "processing 1410\n",
      "processing 1420\n",
      "processing 1430\n",
      "processing 1440\n",
      "processing 1450\n",
      "processing 1460\n",
      "processing 1470\n",
      "processing 1480\n",
      "processing 1490\n"
     ]
    }
   ],
   "source": [
    "get_all_posts('Seattle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(subreddit_name):\n",
    "    #want to figure out how to avoid using full path...but for now here it is\n",
    "    os.chdir(f'/Users/mamabear/Documents/GA-DSI/Projects/project-3/data/{subreddit_name}')\n",
    "    filenames = os.listdir(\".\") #get list of files\n",
    "    filenames = [file for file in filenames if subreddit_name in file] #limit to subreddit\n",
    "    dataframes = [pd.DataFrame(pd.read_csv(file)) for file in filenames]\n",
    "\n",
    "    return pd.concat(dataframes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattlewa = combine_files('SeattleWA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seattle = combine_files('Seattle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59524, 61181)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seattlwa),len(seattle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = seattle['author'].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "#https://stackoverflow.com/questions/47136436/python-pandas-convert-value-counts-output-to-dataframe\n",
    "#answer by jezrael"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counts\n",
       "1       20695\n",
       "2        3912\n",
       "3        1519\n",
       "4         678\n",
       "5         398\n",
       "        ...  \n",
       "72          1\n",
       "73          1\n",
       "76          1\n",
       "83          1\n",
       "3043        1\n",
       "Length: 91, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.value_counts('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
