{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L. Minter\n",
    "### November 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook 01: Data Collection \n",
    "This is the first notebook of the analysis.  It is designed to collect the necessary data using the Pushshift API.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem statement\n",
    "Identify common and disparate themes in the reddit posts to for Portland and Seattle subreddits that could be useful for marketing campaigns across the Pacific Northwest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with a quick look at a single request for r/Seattle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_Seattle= {\n",
    "    'subreddit':'Seattle',\n",
    "    'metadata':'True',\n",
    "    'size':100,\n",
    "    'before':1635439118\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get( url = base_url, params = params_Seattle)\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = res.json()['data']\n",
    "metadata = res.json()['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>...</th>\n",
       "      <th>url_overridden_by_dest</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>is_gallery</th>\n",
       "      <th>gallery_data</th>\n",
       "      <th>media_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>galumphix</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_s4xtg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Automatic_Man52</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_cqpcdsow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>https://datastudio.google.com/reporting/314b52...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>phillipsn21</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_59m8pcz0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moving</td>\n",
       "      <td>Moving / Visiting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>wsdot</td>\n",
       "      <td>flair verified</td>\n",
       "      <td>[{'e': 'text', 't': 'WA State Dept of Transpor...</td>\n",
       "      <td>WA State Dept of Transportation</td>\n",
       "      <td>richtext</td>\n",
       "      <td>t2_4upd9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>dark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>gharrity</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_6lx0n</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/poli...</td>\n",
       "      <td>flair</td>\n",
       "      <td>Politics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d03c04ee-412a-11e8-88cf-0e80e220ed5c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments           author author_flair_css_class  \\\n",
       "0            []                False        galumphix                   None   \n",
       "1            []                False  Automatic_Man52                   None   \n",
       "2            []                False      phillipsn21                   None   \n",
       "3            []                False            wsdot         flair verified   \n",
       "4            []                False         gharrity                   None   \n",
       "\n",
       "                               author_flair_richtext  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [{'e': 'text', 't': 'WA State Dept of Transpor...   \n",
       "4                                                 []   \n",
       "\n",
       "                 author_flair_text author_flair_type author_fullname  \\\n",
       "0                             None              text        t2_s4xtg   \n",
       "1                             None              text     t2_cqpcdsow   \n",
       "2                             None              text     t2_59m8pcz0   \n",
       "3  WA State Dept of Transportation          richtext        t2_4upd9   \n",
       "4                             None              text        t2_6lx0n   \n",
       "\n",
       "   author_is_blocked  author_patreon_flair  ...  \\\n",
       "0              False                 False  ...   \n",
       "1              False                 False  ...   \n",
       "2              False                 False  ...   \n",
       "3              False                 False  ...   \n",
       "4              False                 False  ...   \n",
       "\n",
       "                              url_overridden_by_dest link_flair_css_class  \\\n",
       "0                                                NaN                  NaN   \n",
       "1  https://datastudio.google.com/reporting/314b52...                  NaN   \n",
       "2                                                NaN               moving   \n",
       "3                                                NaN                  NaN   \n",
       "4  https://www.seattletimes.com/seattle-news/poli...                flair   \n",
       "\n",
       "     link_flair_text  author_flair_background_color  author_flair_text_color  \\\n",
       "0                NaN                            NaN                      NaN   \n",
       "1                NaN                            NaN                      NaN   \n",
       "2  Moving / Visiting                            NaN                      NaN   \n",
       "3                NaN                                                    dark   \n",
       "4           Politics                            NaN                      NaN   \n",
       "\n",
       "                 link_flair_template_id author_flair_template_id is_gallery  \\\n",
       "0                                   NaN                      NaN        NaN   \n",
       "1                                   NaN                      NaN        NaN   \n",
       "2                                   NaN                      NaN        NaN   \n",
       "3                                   NaN                      NaN        NaN   \n",
       "4  d03c04ee-412a-11e8-88cf-0e80e220ed5c                      NaN        NaN   \n",
       "\n",
       "  gallery_data  media_metadata  \n",
       "0          NaN             NaN  \n",
       "1          NaN             NaN  \n",
       "2          NaN             NaN  \n",
       "3          NaN             NaN  \n",
       "4          NaN             NaN  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick look at the data coming back\n",
    "seattle = pd.DataFrame(posts)\n",
    "seattle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 100 posts from `r/Seattle`.  Now we can make a function to do this over and over again for specific subreddits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting large numbers of posts at a time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get a bunch of posts from the subreddit we want\n",
    "# input: subreddit_name, \n",
    "# output: a bunch of saved files with 100 posts each, labeled by subreddit name and UTC\n",
    "def get_all_posts(subreddit_name):\n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    starting_utc = 1635439118\n",
    "    \n",
    "    params = {\n",
    "        'subreddit':subreddit_name,\n",
    "        'size':100,\n",
    "        'before':starting_utc\n",
    "        }\n",
    "    \n",
    "    #get initial data frame for first 100 submissions\n",
    "    df = pd.DataFrame(requests.get( url = base_url, params = params).json()['data'])\n",
    "    df.to_csv(f'./data/{subreddit_name}_{starting_utc}.csv')\n",
    "    \n",
    "    #then loop to get the rest of them\n",
    "    for i in range(1500):\n",
    "        if i%10 == 0: print('processing', i) #print a little update so we know how it is going\n",
    "        params['before']=df['created_utc'].min() #update UTC cutoff using data we already have\n",
    "        #print(df['created_utc'].min())\n",
    "        res = requests.get( url = base_url, params = params)\n",
    "        if res.status_code==200:\n",
    "            posts = res.json()['data']\n",
    "            df = pd.DataFrame(posts)\n",
    "            utc = params['before'] #UTC we will use to label file\n",
    "            \n",
    "            #write the 100 posts data frame to file, no index\n",
    "            df.to_csv(f'./data/{subreddit_name}/posts_{subreddit_name}_{utc}.csv',index = False)\n",
    "    \n",
    "        time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0\n",
      "processing 10\n",
      "processing 20\n",
      "processing 30\n",
      "processing 40\n",
      "processing 50\n",
      "processing 60\n",
      "processing 70\n",
      "processing 80\n",
      "processing 90\n",
      "processing 100\n",
      "processing 110\n",
      "processing 120\n",
      "processing 130\n",
      "processing 140\n",
      "processing 150\n",
      "processing 160\n",
      "processing 170\n",
      "processing 180\n",
      "processing 190\n",
      "processing 200\n",
      "processing 210\n",
      "processing 220\n",
      "processing 230\n",
      "processing 240\n",
      "processing 250\n",
      "processing 260\n",
      "processing 270\n",
      "processing 280\n",
      "processing 290\n",
      "processing 300\n",
      "processing 310\n",
      "processing 320\n",
      "processing 330\n",
      "processing 340\n",
      "processing 350\n",
      "processing 360\n",
      "processing 370\n",
      "processing 380\n",
      "processing 390\n",
      "processing 400\n",
      "processing 410\n",
      "processing 420\n",
      "processing 430\n",
      "processing 440\n",
      "processing 450\n",
      "processing 460\n",
      "processing 470\n",
      "processing 480\n",
      "processing 490\n",
      "processing 500\n",
      "processing 510\n",
      "processing 520\n",
      "processing 530\n",
      "processing 540\n",
      "processing 550\n",
      "processing 560\n",
      "processing 570\n",
      "processing 580\n",
      "processing 590\n",
      "processing 600\n",
      "processing 610\n",
      "processing 620\n",
      "processing 630\n",
      "processing 640\n",
      "processing 650\n",
      "processing 660\n",
      "processing 670\n",
      "processing 680\n",
      "processing 690\n",
      "processing 700\n",
      "processing 710\n",
      "processing 720\n",
      "processing 730\n",
      "processing 740\n",
      "processing 750\n",
      "processing 760\n",
      "processing 770\n",
      "processing 780\n",
      "processing 790\n",
      "processing 800\n",
      "processing 810\n",
      "processing 820\n",
      "processing 830\n",
      "processing 840\n",
      "processing 850\n",
      "processing 860\n",
      "processing 870\n",
      "processing 880\n",
      "processing 890\n",
      "processing 900\n",
      "processing 910\n",
      "processing 920\n",
      "processing 930\n",
      "processing 940\n",
      "processing 950\n",
      "processing 960\n",
      "processing 970\n",
      "processing 980\n",
      "processing 990\n",
      "processing 1000\n",
      "processing 1010\n",
      "processing 1020\n",
      "processing 1030\n",
      "processing 1040\n",
      "processing 1050\n",
      "processing 1060\n",
      "processing 1070\n",
      "processing 1080\n",
      "processing 1090\n",
      "processing 1100\n",
      "processing 1110\n",
      "processing 1120\n",
      "processing 1130\n",
      "processing 1140\n",
      "processing 1150\n",
      "processing 1160\n",
      "processing 1170\n",
      "processing 1180\n",
      "processing 1190\n",
      "processing 1200\n",
      "processing 1210\n",
      "processing 1220\n",
      "processing 1230\n",
      "processing 1240\n",
      "processing 1250\n",
      "processing 1260\n",
      "processing 1270\n",
      "processing 1280\n",
      "processing 1290\n",
      "processing 1300\n",
      "processing 1310\n",
      "processing 1320\n",
      "processing 1330\n",
      "processing 1340\n",
      "processing 1350\n",
      "processing 1360\n",
      "processing 1370\n",
      "processing 1380\n",
      "processing 1390\n",
      "processing 1400\n",
      "processing 1410\n",
      "processing 1420\n",
      "processing 1430\n",
      "processing 1440\n",
      "processing 1450\n",
      "processing 1460\n",
      "processing 1470\n",
      "processing 1480\n",
      "processing 1490\n"
     ]
    }
   ],
   "source": [
    "get_all_posts('SeattleWA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0\n",
      "processing 10\n",
      "processing 20\n",
      "processing 30\n",
      "processing 40\n",
      "processing 50\n",
      "processing 60\n",
      "processing 70\n",
      "processing 80\n",
      "processing 90\n",
      "processing 100\n",
      "processing 110\n",
      "processing 120\n",
      "processing 130\n",
      "processing 140\n",
      "processing 150\n",
      "processing 160\n",
      "processing 170\n",
      "processing 180\n",
      "processing 190\n",
      "processing 200\n",
      "processing 210\n",
      "processing 220\n",
      "processing 230\n",
      "processing 240\n",
      "processing 250\n",
      "processing 260\n",
      "processing 270\n",
      "processing 280\n",
      "processing 290\n",
      "processing 300\n",
      "processing 310\n",
      "processing 320\n",
      "processing 330\n",
      "processing 340\n",
      "processing 350\n",
      "processing 360\n",
      "processing 370\n",
      "processing 380\n",
      "processing 390\n",
      "processing 400\n",
      "processing 410\n",
      "processing 420\n",
      "processing 430\n",
      "processing 440\n",
      "processing 450\n",
      "processing 460\n",
      "processing 470\n",
      "processing 480\n",
      "processing 490\n",
      "processing 500\n",
      "processing 510\n",
      "processing 520\n",
      "processing 530\n",
      "processing 540\n",
      "processing 550\n",
      "processing 560\n",
      "processing 570\n",
      "processing 580\n",
      "processing 590\n",
      "processing 600\n",
      "processing 610\n",
      "processing 620\n",
      "processing 630\n",
      "processing 640\n",
      "processing 650\n",
      "processing 660\n",
      "processing 670\n",
      "processing 680\n",
      "processing 690\n",
      "processing 700\n",
      "processing 710\n",
      "processing 720\n",
      "processing 730\n",
      "processing 740\n",
      "processing 750\n",
      "processing 760\n",
      "processing 770\n",
      "processing 780\n",
      "processing 790\n",
      "processing 800\n",
      "processing 810\n",
      "processing 820\n",
      "processing 830\n",
      "processing 840\n",
      "processing 850\n",
      "processing 860\n",
      "processing 870\n",
      "processing 880\n",
      "processing 890\n",
      "processing 900\n",
      "processing 910\n",
      "processing 920\n",
      "processing 930\n",
      "processing 940\n",
      "processing 950\n",
      "processing 960\n",
      "processing 970\n",
      "processing 980\n",
      "processing 990\n",
      "processing 1000\n",
      "processing 1010\n",
      "processing 1020\n",
      "processing 1030\n",
      "processing 1040\n",
      "processing 1050\n",
      "processing 1060\n",
      "processing 1070\n",
      "processing 1080\n",
      "processing 1090\n",
      "processing 1100\n",
      "processing 1110\n",
      "processing 1120\n",
      "processing 1130\n",
      "processing 1140\n",
      "processing 1150\n",
      "processing 1160\n",
      "processing 1170\n",
      "processing 1180\n",
      "processing 1190\n",
      "processing 1200\n",
      "processing 1210\n",
      "processing 1220\n",
      "processing 1230\n",
      "processing 1240\n",
      "processing 1250\n",
      "processing 1260\n",
      "processing 1270\n",
      "processing 1280\n",
      "processing 1290\n",
      "processing 1300\n",
      "processing 1310\n",
      "processing 1320\n",
      "processing 1330\n",
      "processing 1340\n",
      "processing 1350\n",
      "processing 1360\n",
      "processing 1370\n",
      "processing 1380\n",
      "processing 1390\n",
      "processing 1400\n",
      "processing 1410\n",
      "processing 1420\n",
      "processing 1430\n",
      "processing 1440\n",
      "processing 1450\n",
      "processing 1460\n",
      "processing 1470\n",
      "processing 1480\n",
      "processing 1490\n"
     ]
    }
   ],
   "source": [
    "get_all_posts('Seattle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_posts('Portland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the files into single file for each subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(subreddit_name):\n",
    "    #want to figure out how to avoid using full path...but for now here it is\n",
    "    os.chdir(f'/Users/mamabear/Documents/GA-DSI/Projects/project-3/data/{subreddit_name}')\n",
    "    filenames = os.listdir(\".\") #get list of files\n",
    "    filenames = [file for file in filenames if subreddit_name in file] #limit to subreddit\n",
    "    dataframes = [pd.DataFrame(pd.read_csv(file)) for file in filenames]\n",
    "\n",
    "    return pd.concat(dataframes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattlewa = combine_files('SeattleWA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seattle = combine_files('Seattle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portland = combine_files('Portland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59524, 61181)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seattlwa),len(seattle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "Now that we are done getting the data we can move on to data cleaning using the next [notebook](./02_data_cleaning.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
